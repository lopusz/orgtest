* Issues
  + Clojars badge test

    [[http://clojars.org/langlab][http://clojars.org/langlab/latest-version.svg]]
  + Table test
    | Name | Description |
    |------+-------------|
    | John | test desc.  |
    | Jane | empty desc. |
    | Alice| full desc.  |
  + What is a difference between verbatim and code
    =code(call,call,...)=, ~verbatim(verbatim,verbatim,...)~
  + This is verbatim test

    #+BEGIN_SRC text
    > ls -1 | less
    > pwd
    #+END_SRC
  
  + Another code block
    
    #+BEGIN_SRC clojure
    (+ 2 2)
    #+END_SRC
  
  + *[detectors]* Lookup new method from Cybozu for tweeets
   http://shuyo.wordpress.com/2012/05/17/short-text-language-detection-with-infinity-gram/
  + *[detectors]* Add tests for language detection.
  + *detectors* Add tests for encoding.
  + *detectors* Add option of seed initialization fro Cybozu lang detector
  + *general* Check for ICU automatic detection + decoding facilities
    This may solve the probles of different encodings in files.
  + *detectors* Investigage the issue of ICU language detection problems
    Why it works only for texts with removed diacritics?
    Does manual removal affect the non-latin alphabet text detection?
  + *characters* Add function to detect/normalize some UTF-8 characters
    Examples
    - "…" -> "..."
    - "„" -> "\""
    - "”" -> "\""
    - "—" -> "-"
    - hard spaces to spaces
    - what more ?

  + *stopwords* Create a module and add general stopwords filter
  + *stopwords* Convert en-drop-articles so it uses stopwords with
  + *detectors* Create wrappers for language detection module in Apache Tika
  + *tagging*   Refactor tagging module so it works with langlab
* Done
  + *general* Refactor langlab-base -> langlab
  + *readability* Correct test to the infix notation
  + *readablilty* Correct counting characters to bi version
  + *characters* Add functions detecting/removing non-MBP characters
  + *transformers* Add tests to transformers
  + *parsers* Add tests to sentence splitters
  + *parsers* Add simple tokenizer based on Analyzer from Lucene
    http://stackoverflow.com/questions/6334692/how-to-use-a-lucene-analyzer-to-tokenize-a-string
    See post by Ben McCann for Lucene 4.1
  + *characters* Make use of punctuation classes from Unicode
    [Pc] Punctuation, Connector
    [Pd] Punctuation, Dash
    [Pe] Punctuation, Close
    [Pf] Punctuation, Final quote (may behave like Ps or Pe depending on usage)
    [Pi] Punctuation, Initial quote (may behave like Ps or Pe depending on usage)
    [Po] Punctuation, Other
    [Ps] Punctuation, Open
     see http://www.fileformat.info/info/unicode/category/index.htm
  + *characters* Add tokenizer based on module ICU4j and its break iterator
     It implements unicode segmentatior rules http://www.unicode.org/reports/tr29/
     http://icu-project.org/apiref/icu4j/com/ibm/icu/text/BreakIterator.html
     More http://site.icu-project.org/
  + *characters* Add Java functions to characters module
    containsPunctuation(String s)
    containsPunctuationOnly(String s)
    containsWhitespace(String s)
    containsWhitespaceOnly(String s)
